{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain states transitions in response to working memory training\n",
    "=================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyses performed by Karolina Finc & Justyna Kuk, \n",
    "*Centre for Modern interdisciplinary Technologies, Nicolaus Copernicus University in Toru≈Ñ*\n",
    "\n",
    "Last edited: 08-05-2019\n",
    "\n",
    "--------------\n",
    "\n",
    "The goal of this analysis is to examine the effects of working memory training on *time-resolved brain state dynamics* examined in the trained task and the resting-state. Does working memory training affects brain state distribution in the task? \n",
    "\n",
    "We used unsupervised machine learning approach to cluster dual n-back and resting-state fMRI time-series into discrete brain states (see Chen et al., 2015; Cornblath et al., 2018). We hypothesize that:\n",
    "- The brain states fluctuation will differ after working memory training especially in the states related to default mode and frontoparietal systems activity. \n",
    "- Training-related changes in brain states dynamics will be more visible during the trained task than the resting state.\n",
    "- Individual characteristics of brain states fluctuations will be associated to individual behavioral differences in training progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Timeseries preparation\n",
    "----------\n",
    "\n",
    "Prior to running clustering the time-series into discrete brain stets, all timeseries were concatenated into large $N \\times P$ array containing $N$ observation and $P$ features. The length of $N$ was equal to 227040 as a result of concatenating 4 sessions of dual n-back data (340 time-points) and resting state data (305 time-points) of 44 subjects. The length of $P$ was equal 400 and represented the mean signal extracted from each brain areas of Schaefer et al. (2019) brain parcellation.\n",
    "\n",
    "By this procedure we ensured the correspondence of brain states labels across subjects, sessions and tasks.\n",
    "\n",
    "(for now, we just testing code on small subsample of dataset -- 10 subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 4, 340, 264)\n",
      "(46, 4, 305, 264)\n"
     ]
    }
   ],
   "source": [
    "#from nilearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "n_sub = 46\n",
    "# power = datasets.fetch_coords_power_2011()\n",
    "# coords = np.vstack((power.rois['x'], power.rois['y'], power.rois['z'])).T\n",
    "data = np.load(\"LB_dualnback_timeseries.npy\")\n",
    "data2 = np.load(\"LB_rest_timeseries.npy\")\n",
    "\n",
    "time_series_nback = data[0:n_sub]\n",
    "time_series_rest = data2[0:n_sub]\n",
    "print(time_series_nback.shape)\n",
    "print(time_series_rest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectors of time-series:\n",
    "\n",
    "t_nback = time_series_nback.reshape(n_sub,1360,264)    #for each subject\n",
    "t_rest = time_series_rest.reshape(n_sub,1220,264)\n",
    "\n",
    "t_vector_nback = time_series_nback.reshape(n_sub*4*340, 264)     #all 46 subcjects in one vector\n",
    "t_vector_rest = time_series_rest.reshape(n_sub*4*305, 264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one vector for both rest and nback time-series:\n",
    "\n",
    "t_vector = np.zeros((118680, 264))\n",
    "t_vector[0:62560,:] = t_vector_nback\n",
    "t_vector[62560:, :] = t_vector_rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Clustering the timeseries into brain states\n",
    "----------------------\n",
    "\n",
    "To discover main brain states existing in time-series we performed 500 repetitions of $k$-means clustering from $k$ = 2 to $k$ = 18 using Euclidean distance as a measure of similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 17\n",
    "group_k_labels = np.zeros((n_clusters,t_vector.shape[0]))\n",
    "\n",
    "for k in range(n_clusters):\n",
    "        kmeans = KMeans(n_clusters=k+2, n_init=100).fit(t_vector)\n",
    "        k_labels = kmeans.labels_\n",
    "        print(k_labels)\n",
    "        group_k_labels[k,:] = k_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Selecting k size based on silhouette score and absent states\n",
    "-------------------------------------------------------------------------\n",
    "To identify the optimal number of clusters we use silhouette score...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import number_of_clusters\n",
    "\n",
    "number_of_clusters(8,t_vector_nback, n_init=500)  \n",
    "number_of_clusters(8,t_vector_rest, n_init=500)  \n",
    "number_of_clusters(8,t_vector, n_init=500)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Split-halves validation for k = X\n",
    "-------------------------------------------------------------------------\n",
    "\n",
    "We also split our sample into two equal partitions 500 times and performed k-means clustering separately on each half of the dataset. We then matched clusters by computing the correlation between both sets of centroids, and then by reordering the clusters based on the maximum correlation value for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dim = int(t_vector.shape[0]/2)\n",
    "train_series = np.zeros((500,dim,264))\n",
    "test_series = np.zeros((500,dim,264))\n",
    "\n",
    "for i in range(500):\n",
    "    X_train, X_test = train_test_split(t_vector, test_size=0.5)\n",
    "    train_series[i,:,:] = X_train\n",
    "    test_series[i,:,:] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "n_clusters = 6\n",
    "\n",
    "cor = np.zeros((500,n_clusters,n_clusters))\n",
    "\n",
    "for sub in range(500):\n",
    "    train_kmeans = KMeans(n_clusters=n_clusters, n_init=500).fit(train_series[sub,:,:])\n",
    "    test_kmeans = KMeans(n_clusters=n_clusters, n_init=500).fit(test_series[sub,:,:])\n",
    "    x = train_kmeans.cluster_centers_\n",
    "    y = test_kmeans.cluster_centers_\n",
    "    for k in range(n_clusters):\n",
    "        for j in range(n_clusters):\n",
    "            cor[sub,k,j] = pearsonr(x[k],y[j])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_sort = np.sort(cor)\n",
    "max_values = np.zeros((500,n_clusters))\n",
    "\n",
    "for i in range(500):\n",
    "    max_values = cor_sort[i,:,-1]\n",
    "    mean_max_values = (cor_sort[i,:,-1]).mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "\n",
    "num = 0\n",
    "\n",
    "print(cor_sort[num,:,-1])\n",
    "plotting.plot_matrix(cor_sort[num,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Correlating cluster labels with well-known large-scale networks\n",
    "-------------------------------------------------------------------------\n",
    "We calculate corralation between mean of the time-series for each cluster and modules using Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import compare\n",
    "compare(t_vector,n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Calculating persistence probablilities, transition probablilities, dwell times for each subject\n",
    "-------------------------------------------------------------------------\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition probabilities\n",
    "\n",
    "from nilearn import plotting\n",
    "import functions \n",
    "\n",
    "plotting.plot_matrix(functions.multiple_transition(n_clusters,t_rest).mean(axis=0), title = \"rest\")\n",
    "plotting.plot_matrix(functions.multiple_transition(n_clusters,t_nback).mean(axis=0), title = \"nback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Groups/sessions comparison\n",
    "-------------------------------------------------------------------------\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
